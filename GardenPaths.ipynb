{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd00731369e360385e76366cf827f604697773b9d9d66be5c37ba6a191781d1c178",
   "display_name": "Python 3.8.8 64-bit ('691DD': conda)"
  },
  "interpreter": {
   "hash": "07de30308711806ec6062f7254ded7134ae2bc160a01411f26624d89ff5a5026"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import re\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def print_header(header):\n",
    "    print(\"\\n\\n----------------------------------------------------------\")\n",
    "    print(header)\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "models = {\n",
    "    # 'BERT': 'bert-base-uncased',\n",
    "    'BERT (whole word)': 'bert-large-uncased-whole-word-masking',\n",
    "    'GPT': 'gpt2',\n",
    "}\n",
    "\n",
    "horse_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': \"The horse raced past the barn [MASK]\",\n",
    "        'un-ambiguated': \"The horse raced past the barn, [MASK]\"\n",
    "    }\n",
    "}\n",
    "\n",
    "horse_fillers = [\n",
    "        'fell',\n",
    "        'is',\n",
    "        'was',\n",
    "        'and',\n",
    "        '.',\n",
    "    ]\n",
    "\n",
    "butter_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': \"The butter melted in the pan [MASK]\",\n",
    "        'un-ambiguated': \"The butter melted in the pan, [MASK]\",\n",
    "    }\n",
    "}\n",
    "\n",
    "butter_fillers = [\n",
    "        'smelled',\n",
    "        'smells',\n",
    "        'is',\n",
    "        'was',\n",
    "        'and',\n",
    "        '.',\n",
    "    ]\n",
    "\n",
    "politician_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': 'The corrupt politician mentioned the bill [MASK]',\n",
    "        'un-ambiguated': 'The corrupt politician that mentioned the bill [MASK]',\n",
    "    },\n",
    "    'NP/Z': {\n",
    "        'ambiguous': 'After the corrupt politician signed the bill [MASK]',\n",
    "        'un-ambiguated': 'After the corrupt politician signed, the bill [MASK]',\n",
    "    },\n",
    "    'MVRR': {\n",
    "        'ambiguous': 'The corrupt politician handed the bill [MASK]',\n",
    "        'un-ambiguated': 'The corrupt politician who was handed the bill [MASK]',\n",
    "    },\n",
    "}\n",
    "\n",
    "politician_fillers = {\n",
    "    'incorrect': [\n",
    "            'and',\n",
    "            '.',\n",
    "            'to',\n",
    "    ],\n",
    "    'correct': [\n",
    "        'is',\n",
    "        'was',\n",
    "        'received'\n",
    "    ],\n",
    "}\n",
    "\n",
    "politician_filler = 'received'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Exploration -- BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def setup(model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model])\n",
    "    model = AutoModelForMaskedLM.from_pretrained(models[model])\n",
    "    bert = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "    mask = bert.tokenizer.mask_token\n",
    "    return bert, mask\n",
    "\n",
    "def runBERT(bert, sentence_dict, fillers):\n",
    "    filler_results = {}\n",
    "    top_preds = {}\n",
    "    for sentence_type in sentence_dict:\n",
    "        top_preds[sentence_type] = {}\n",
    "        filler_results[sentence_type] = {}\n",
    "        for clarity in ['ambiguous', 'un-ambiguated']:\n",
    "            sentence = sentence_dict[sentence_type][clarity]\n",
    "            outputs = bert(sentence, top_k=12)\n",
    "            top_preds[sentence_type][clarity] = [(output[\"token_str\"], output['score']) for output in outputs]\n",
    "            filler_results[sentence_type][clarity] = {}\n",
    "            for accuracy in fillers:\n",
    "                for filler in fillers[accuracy]:\n",
    "                    filler_results[sentence_type][clarity][filler] = bert(sentence, targets=[filler])[0][\"score\"]\n",
    "    return filler_results, top_preds\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "bert, mask = setup('BERT (whole word)')\n",
    "filler_results, top_preds = runBERT(bert, politician_sentence_dict, politician_fillers)\n",
    "\n",
    "print_header('Filler Results')\n",
    "pprint(filler_results, sort_dicts=False)\n",
    "\n",
    "# print_header('Model Predictions')\n",
    "# pprint(top_preds, sort_dicts=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "----------------------------------------------------------\n",
      "Filler Results\n",
      "----------------------------------------------------------\n",
      "{'NP/S': {'ambiguous': {'and': 0.001315354835242033,\n",
      "                        '.': 0.7417080998420715,\n",
      "                        'to': 0.002144616562873125,\n",
      "                        'received': 2.3071579562383704e-06},\n",
      "          'un-ambiguated': {'and': 0.00040642396197654307,\n",
      "                            '.': 0.22318482398986816,\n",
      "                            'to': 0.0011608798522502184,\n",
      "                            'received': 2.1722256860812195e-05}},\n",
      " 'NP/Z': {'ambiguous': {'and': 0.0007047720719128847,\n",
      "                        '.': 0.7722906470298767,\n",
      "                        'to': 0.0003126610827166587,\n",
      "                        'received': 8.955280463851523e-06},\n",
      "          'un-ambiguated': {'and': 0.0014642988098785281,\n",
      "                            '.': 0.17061227560043335,\n",
      "                            'to': 0.006601774133741856,\n",
      "                            'received': 0.00045881321420893073}},\n",
      " 'MVRR': {'ambiguous': {'and': 0.0004763128818012774,\n",
      "                        '.': 0.9299089312553406,\n",
      "                        'to': 0.0025956332683563232,\n",
      "                        'received': 5.374031388782896e-07},\n",
      "          'un-ambiguated': {'and': 0.0004215073131490499,\n",
      "                            '.': 0.7884126901626587,\n",
      "                            'to': 0.00071937550092116,\n",
      "                            'received': 8.466547114949208e-06}}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def get_ratios(filler_results, filler):\n",
    "    ratios = {}\n",
    "    for sentence_type in filler_results:\n",
    "        unambig = filler_results[sentence_type]['un-ambiguated'][filler]\n",
    "        ambig = filler_results[sentence_type]['ambiguous'][filler]\n",
    "        ratios[sentence_type] = unambig/ambig\n",
    "    return ratios\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "ratios = get_ratios(filler_results, politician_filler)\n",
    "pprint(ratios, sort_dicts=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'NP/S': 9.415158074494618,\n",
      " 'NP/Z': 51.23381853432232,\n",
      " 'MVRR': 15.754554639597483}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Exploration -- GPT2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np \n",
    "\n",
    "def score(model, tokens_tensor):\n",
    "    loss=model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "    return np.exp(loss.cpu().detach().numpy())\n",
    "\n",
    "def runGPT(sentence_dict):\n",
    "    model = GPT2LMHeadModel.from_pretrained(models['GPT'])\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(models['GPT'])\n",
    "    filler_results = {}\n",
    "    top_preds = {}\n",
    "    model.eval()\n",
    "    for sentence_type in sentence_dict:\n",
    "        for sentence in sentence_type['sentences']:\n",
    "            # text = sentence.replace('[MASK]', '')\n",
    "            # with torch.no_grad():\n",
    "                # outputs = model(tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\"))\n",
    "                # predictions = outputs[0][0, -1, :]\n",
    "                # print(len(predictions))\n",
    "                # print([tokenizer.decode([pred.item()]) for pred in predictions])\n",
    "\n",
    "            # next_token_logits = outputs[0]\n",
    "            # print(next_token_logits)\n",
    "            \n",
    "            filler_results[sentence] = {}\n",
    "            for filler in sentence_type['fillers']:\n",
    "                tokens_tensor = tokenizer.encode(sentence.replace('[MASK]', filler), add_special_tokens=False, return_tensors=\"pt\")\n",
    "                filler_results[sentence][filler] = score(model, tokens_tensor)\n",
    "    return filler_results, top_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# filler_results, top_preds = runGPT(sentence_dict)\n",
    "# print_header(\"Filler Results\")\n",
    "# pprint(filler_results, sort_dicts=False)\n",
    "# print_header(\"Model Predictions\")\n",
    "# pprint(top_preds, sort_dicts=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}