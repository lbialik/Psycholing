{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd00731369e360385e76366cf827f604697773b9d9d66be5c37ba6a191781d1c178",
   "display_name": "Python 3.8.8 64-bit ('691DD': conda)"
  },
  "interpreter": {
   "hash": "07de30308711806ec6062f7254ded7134ae2bc160a01411f26624d89ff5a5026"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import re\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def print_header(header):\n",
    "    print(\"\\n\\n----------------------------------------------------------\")\n",
    "    print(header)\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "models = {\n",
    "    # 'BERT': 'bert-base-uncased',\n",
    "    'BERT (whole word)': 'bert-large-uncased-whole-word-masking',\n",
    "    'GPT': 'gpt2',\n",
    "}\n",
    "\n",
    "horse_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': \"The horse raced past the barn [MASK]\",\n",
    "        'un-ambiguated': \"The horse raced past the barn, [MASK]\"\n",
    "    }\n",
    "}\n",
    "\n",
    "horse_fillers = [\n",
    "        'fell',\n",
    "        'is',\n",
    "        'was',\n",
    "        'and',\n",
    "        '.',\n",
    "    ]\n",
    "\n",
    "butter_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': \"The butter melted in the pan [MASK]\",\n",
    "        'un-ambiguated': \"The butter melted in the pan, [MASK]\",\n",
    "    }\n",
    "}\n",
    "\n",
    "butter_fillers = [\n",
    "        'smelled',\n",
    "        'smells',\n",
    "        'is',\n",
    "        'was',\n",
    "        'and',\n",
    "        '.',\n",
    "    ]\n",
    "\n",
    "politician_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': 'The corrupt politician mentioned the bill [MASK]',\n",
    "        'un-ambiguated': 'The corrupt politician that mentioned the bill [MASK]',\n",
    "    },\n",
    "    'NP/Z': {\n",
    "        'ambiguous': 'After the corrupt politician signed the bill [MASK]',\n",
    "        'un-ambiguated': 'After the corrupt politician signed, the bill [MASK]',\n",
    "    },\n",
    "    'MVRR': {\n",
    "        'ambiguous': 'The corrupt politician handed the bill [MASK]',\n",
    "        'un-ambiguated': 'The corrupt politician who was handed the bill [MASK]',\n",
    "    },\n",
    "}\n",
    "\n",
    "politician_fillers = {\n",
    "    'incorrect': [\n",
    "            'and',\n",
    "            '.',\n",
    "            'to',\n",
    "    ],\n",
    "    'correct': [\n",
    "        'is',\n",
    "        'was',\n",
    "        'received'\n",
    "    ],\n",
    "}\n",
    "\n",
    "bert, mask = setup('BERT (whole word)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Exploration -- BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "def setup(model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model])\n",
    "    model = AutoModelForMaskedLM.from_pretrained(models[model])\n",
    "    bert = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "    mask = bert.tokenizer.mask_token\n",
    "    return bert, mask\n",
    "\n",
    "def runBERT(bert, sentence_dict, fillers):\n",
    "    filler_results = {}\n",
    "    top_preds = {}\n",
    "    for sentence_type in sentence_dict:\n",
    "        top_preds[sentence_type] = {}\n",
    "        filler_results[sentence_type] = {}\n",
    "        for clarity in ['ambiguous', 'un-ambiguated']:\n",
    "            sentence = sentence_dict[sentence_type][clarity]\n",
    "            outputs = bert(sentence, top_k=12)\n",
    "            top_preds[sentence_type][clarity] = [(output[\"token_str\"], output['score']) for output in outputs]\n",
    "            filler_results[sentence_type][clarity] = {}\n",
    "            for accuracy in fillers:\n",
    "                filler_results[sentence_type][clarity][accuracy] = {}\n",
    "                for filler in fillers[accuracy]:\n",
    "                    filler_results[sentence_type][clarity][accuracy][filler] = bert(sentence, targets=[filler])[0][\"score\"]\n",
    "    return filler_results, top_preds\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "filler_results, top_preds = runBERT(bert, politician_sentence_dict, politician_fillers)\n",
    "\n",
    "print_header('Filler Results')\n",
    "pprint(filler_results, sort_dicts=False)\n",
    "\n",
    "# print_header('Model Predictions')\n",
    "# pprint(top_preds, sort_dicts=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----------------------------------------------------------\nFiller Results\n----------------------------------------------------------\n{'NP/S': {'ambiguous': {'incorrect': {'and': 0.0013153573963791132,\n                                      '.': 0.7417082190513611,\n                                      'to': 0.0021446088794618845},\n                        'correct': {'is': 0.0003153316501993686,\n                                    'was': 0.0009200123022310436,\n                                    'received': 2.3071584109857213e-06}},\n          'un-ambiguated': {'incorrect': {'and': 0.00040642256499268115,\n                                          '.': 0.22318512201309204,\n                                          'to': 0.0011608753120526671},\n                            'correct': {'is': 0.05572093278169632,\n                                        'was': 0.15602436661720276,\n                                        'received': 2.1722347810282372e-05}}},\n 'NP/Z': {'ambiguous': {'incorrect': {'and': 0.0007047722465358675,\n                                      '.': 0.7722893953323364,\n                                      'to': 0.0003126620431430638},\n                        'correct': {'is': 7.207292946986854e-05,\n                                    'was': 0.0005894345813430846,\n                                    'received': 8.955393241194542e-06}},\n          'un-ambiguated': {'incorrect': {'and': 0.0014642965979874134,\n                                          '.': 0.17061203718185425,\n                                          'to': 0.006601751782000065},\n                            'correct': {'is': 0.015060055069625378,\n                                        'was': 0.2865143120288849,\n                                        'received': 0.0004588125448208302}}},\n 'MVRR': {'ambiguous': {'incorrect': {'and': 0.00047631331835873425,\n                                      '.': 0.9299089312553406,\n                                      'to': 0.0025956383906304836},\n                        'correct': {'is': 2.3748295916448114e-06,\n                                    'was': 1.7000043953885324e-05,\n                                    'received': 5.374010925152106e-07}},\n          'un-ambiguated': {'incorrect': {'and': 0.000421508913859725,\n                                          '.': 0.7884126901626587,\n                                          'to': 0.0007193789351731539},\n                            'correct': {'is': 0.0029977497179061174,\n                                        'was': 0.01258365623652935,\n                                        'received': 8.466506187687628e-06}}}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratios(filler_results, fillers):\n",
    "    ratios = {}\n",
    "    for sentence_type in filler_results:\n",
    "        ratios[sentence_type] = {}\n",
    "        for clarity in ['ambiguous', 'un-ambiguated']:\n",
    "            correct_mean = 0\n",
    "            incorrect_mean = 0\n",
    "            for filler in fillers['correct']:\n",
    "                correct_mean += filler_results[sentence_type][clarity]['correct'][filler]\n",
    "            correct_mean /= len(filler_results[sentence_type][clarity]['correct'])\n",
    "            for filler in fillers['incorrect']:\n",
    "                incorrect_mean += filler_results[sentence_type][clarity]['incorrect'][filler]\n",
    "            incorrect_mean /= len(filler_results[sentence_type][clarity]['incorrect'])\n",
    "            ratios[sentence_type][clarity] = correct_mean/incorrect_mean\n",
    "    return ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'MVRR': {'ambiguous': 2.1342639492010513e-05,\n          'un-ambiguated': 0.01974517359528995},\n 'NP/S': {'ambiguous': 0.001660901706770999,\n          'un-ambiguated': 0.9422235446907513},\n 'NP/Z': {'ambiguous': 0.0008670076072932954,\n          'un-ambiguated': 1.6903761795611767}}\n"
     ]
    }
   ],
   "source": [
    "ratios = get_ratios(filler_results, politician_fillers)\n",
    "pprint(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Exploration -- GPT2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np \n",
    "\n",
    "def score(model, tokens_tensor):\n",
    "    loss=model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "    return np.exp(loss.cpu().detach().numpy())\n",
    "\n",
    "def runGPT(sentence_dict):\n",
    "    model = GPT2LMHeadModel.from_pretrained(models['GPT'])\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(models['GPT'])\n",
    "    filler_results = {}\n",
    "    top_preds = {}\n",
    "    model.eval()\n",
    "    for sentence_type in sentence_dict:\n",
    "        for sentence in sentence_type['sentences']:\n",
    "            # text = sentence.replace('[MASK]', '')\n",
    "            # with torch.no_grad():\n",
    "                # outputs = model(tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\"))\n",
    "                # predictions = outputs[0][0, -1, :]\n",
    "                # print(len(predictions))\n",
    "                # print([tokenizer.decode([pred.item()]) for pred in predictions])\n",
    "\n",
    "            # next_token_logits = outputs[0]\n",
    "            # print(next_token_logits)\n",
    "            \n",
    "            filler_results[sentence] = {}\n",
    "            for filler in sentence_type['fillers']:\n",
    "                tokens_tensor = tokenizer.encode(sentence.replace('[MASK]', filler), add_special_tokens=False, return_tensors=\"pt\")\n",
    "                filler_results[sentence][filler] = score(model, tokens_tensor)\n",
    "    return filler_results, top_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "filler_results, top_preds = runGPT(sentence_dict)\n",
    "print_header(\"Filler Results\")\n",
    "pprint(filler_results, sort_dicts=False)\n",
    "print_header(\"Model Predictions\")\n",
    "pprint(top_preds, sort_dicts=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----------------------------------------------------------\nFiller Results\n----------------------------------------------------------\n{'The corrupt politician mentioned the bill [MASK]': {'is': 723.888,\n                                                      'was': 617.42737,\n                                                      'and': 574.95264,\n                                                      '.': 1431.8153,\n                                                      'to': 592.2486,\n                                                      'received': 1414.098},\n 'The corrupt politician that mentioned the bill [MASK]': {'is': 538.2617,\n                                                           'was': 508.98694,\n                                                           'and': 585.8596,\n                                                           '.': 1188.5912,\n                                                           'to': 596.9901,\n                                                           'received': 1133.8853},\n 'After the corrupt politician signed the bill [MASK]': {'is': 222.96799,\n                                                         'was': 190.7782,\n                                                         'and': 125.204796,\n                                                         '.': 323.2613,\n                                                         'to': 125.716484,\n                                                         'received': 361.25812},\n 'After the corrupt politician signed, the bill [MASK]': {'is': 168.80563,\n                                                          'was': 123.31906,\n                                                          'and': 226.08734,\n                                                          '.': 397.7348,\n                                                          'to': 217.28883,\n                                                          'received': 214.79462},\n 'The corrupt politician handed the bill [MASK]': {'is': 1001.32806,\n                                                   'was': 1096.115,\n                                                   'and': 556.5241,\n                                                   '.': 1347.8455,\n                                                   'to': 232.68866,\n                                                   'received': 1855.3574},\n 'The corrupt politician handed the bill later [MASK]': {'is': 1501.72,\n                                                         'was': 1109.7532,\n                                                         'and': 719.0449,\n                                                         '.': 1623.7888,\n                                                         'to': 606.14703,\n                                                         'received': 1570.9684}}\n\n\n----------------------------------------------------------\nModel Predictions\n----------------------------------------------------------\n{}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}