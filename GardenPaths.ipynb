{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd00731369e360385e76366cf827f604697773b9d9d66be5c37ba6a191781d1c178",
   "display_name": "Python 3.8.8 64-bit ('691DD': conda)"
  },
  "interpreter": {
   "hash": "07de30308711806ec6062f7254ded7134ae2bc160a01411f26624d89ff5a5026"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "source": [
    "import transformers\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "import torch\n",
    "from pprint import pprint\n",
    "import logging\n",
    "import re\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def print_header(header):\n",
    "    print(\"\\n\\n----------------------------------------------------------\")\n",
    "    print(header)\n",
    "    print(\"----------------------------------------------------------\")\n",
    "\n",
    "models = {\n",
    "    # 'BERT': 'bert-base-uncased',\n",
    "    'BERT (whole word)': 'bert-large-uncased-whole-word-masking',\n",
    "    'GPT': 'gpt2',\n",
    "}\n",
    "\n",
    "horse_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': \"The horse raced past the barn [MASK]\",\n",
    "        'un-ambiguated': \"The horse raced past the barn, [MASK]\"\n",
    "    }\n",
    "}\n",
    "\n",
    "horse_fillers = [\n",
    "        'fell',\n",
    "        'is',\n",
    "        'was',\n",
    "        'and',\n",
    "        '.',\n",
    "    ]\n",
    "\n",
    "butter_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': \"The butter melted in the pan [MASK]\",\n",
    "        'un-ambiguated': \"The butter melted in the pan, [MASK]\",\n",
    "    }\n",
    "}\n",
    "\n",
    "butter_fillers = [\n",
    "        'smelled',\n",
    "        'smells',\n",
    "        'is',\n",
    "        'was',\n",
    "        'and',\n",
    "        '.',\n",
    "    ]\n",
    "\n",
    "politician_sentence_dict = {\n",
    "    'NP/S': {\n",
    "        'ambiguous': 'The corrupt politician mentioned the bill [MASK]',\n",
    "        'un-ambiguated': 'The corrupt politician that mentioned the bill [MASK]',\n",
    "    },\n",
    "    'NP/Z': {\n",
    "        'ambiguous': 'After the corrupt politician signed the bill [MASK]',\n",
    "        'un-ambiguated': 'After the corrupt politician signed, the bill [MASK]',\n",
    "    },\n",
    "    'MVRR': {\n",
    "        'ambiguous': 'The corrupt politician handed the bill [MASK]',\n",
    "        'un-ambiguated': 'The corrupt politician who was handed the bill [MASK]',\n",
    "    },\n",
    "}\n",
    "\n",
    "politician_fillers = {\n",
    "    'incorrect': [\n",
    "            'and',\n",
    "            '.',\n",
    "            'to',\n",
    "    ],\n",
    "    'correct': [\n",
    "        'is',\n",
    "        'was',\n",
    "        'received'\n",
    "    ],\n",
    "}\n",
    "\n",
    "bert, mask = setup('BERT (whole word)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Exploration -- BERT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "def setup(model):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(models[model])\n",
    "    model = AutoModelForMaskedLM.from_pretrained(models[model])\n",
    "    bert = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "    mask = bert.tokenizer.mask_token\n",
    "    return bert, mask\n",
    "\n",
    "def runBERT(bert, sentence_dict, fillers):\n",
    "    filler_results = {}\n",
    "    top_preds = {}\n",
    "    for sentence_type in sentence_dict:\n",
    "        top_preds[sentence_type] = {}\n",
    "        filler_results[sentence_type] = {}\n",
    "        for clarity in ['ambiguous', 'un-ambiguated']:\n",
    "            sentence = sentence_dict[sentence_type][clarity]\n",
    "            outputs = bert(sentence, top_k=12)\n",
    "            top_preds[sentence_type][clarity] = [(output[\"token_str\"], output['score']) for output in outputs]\n",
    "            filler_results[sentence_type][clarity] = {}\n",
    "            for accuracy in fillers:\n",
    "                filler_results[sentence_type][clarity][accuracy] = {}\n",
    "                for filler in fillers[accuracy]:\n",
    "                    filler_results[sentence_type][clarity][accuracy][filler] = bert(sentence, targets=[filler])[0][\"score\"]\n",
    "    return filler_results, top_preds\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "source": [
    "filler_results, top_preds = runBERT(bert, politician_sentence_dict, politician_fillers)\n",
    "\n",
    "print_header('Filler Results')\n",
    "pprint(filler_results, sort_dicts=False)\n",
    "\n",
    "print_header('Model Predictions')\n",
    "pprint(top_preds, sort_dicts=False)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----------------------------------------------------------\nFiller Results\n----------------------------------------------------------\n{'NP/S': {'ambiguous': {'incorrect': {'and': 0.0013153573963791132,\n                                      '.': 0.7417082190513611,\n                                      'to': 0.0021446088794618845},\n                        'correct': {'is': 0.0003153316501993686,\n                                    'was': 0.0009200123022310436,\n                                    'received': 2.3071584109857213e-06}},\n          'un-ambiguated': {'incorrect': {'and': 0.00040642256499268115,\n                                          '.': 0.22318512201309204,\n                                          'to': 0.0011608753120526671},\n                            'correct': {'is': 0.05572093278169632,\n                                        'was': 0.15602436661720276,\n                                        'received': 2.1722347810282372e-05}}},\n 'NP/Z': {'ambiguous': {'incorrect': {'and': 0.0007047722465358675,\n                                      '.': 0.7722893953323364,\n                                      'to': 0.0003126620431430638},\n                        'correct': {'is': 7.207292946986854e-05,\n                                    'was': 0.0005894345813430846,\n                                    'received': 8.955393241194542e-06}},\n          'un-ambiguated': {'incorrect': {'and': 0.0014642965979874134,\n                                          '.': 0.17061203718185425,\n                                          'to': 0.006601751782000065},\n                            'correct': {'is': 0.015060055069625378,\n                                        'was': 0.2865143120288849,\n                                        'received': 0.0004588125448208302}}},\n 'MVRR': {'ambiguous': {'incorrect': {'and': 0.00047631331835873425,\n                                      '.': 0.9299089312553406,\n                                      'to': 0.0025956383906304836},\n                        'correct': {'is': 2.3748295916448114e-06,\n                                    'was': 1.7000043953885324e-05,\n                                    'received': 5.374010925152106e-07}},\n          'un-ambiguated': {'incorrect': {'and': 0.000421508913859725,\n                                          '.': 0.7884126901626587,\n                                          'to': 0.0007193789351731539},\n                            'correct': {'is': 0.0029977497179061174,\n                                        'was': 0.01258365623652935,\n                                        'received': 8.466506187687628e-06}}}}\n\n\n----------------------------------------------------------\nModel Predictions\n----------------------------------------------------------\n{'NP/S': {'ambiguous': [('.', 0.7417082190513611),\n                        (':', 0.1526390165090561),\n                        (';', 0.03662469983100891),\n                        ('?', 0.015996668487787247),\n                        (',', 0.01324289757758379),\n                        ('as', 0.009066874161362648),\n                        ('...', 0.0040427204221487045),\n                        ('!', 0.0032828303519636393),\n                        ('to', 0.0021446088794618845),\n                        ('and', 0.0013153573963791132),\n                        ('of', 0.0011719432659447193),\n                        ('was', 0.0009200123022310436)],\n          'un-ambiguated': [(':', 0.4416533410549164),\n                            ('.', 0.22318512201309204),\n                            ('was', 0.15602436661720276),\n                            ('is', 0.05572093278169632),\n                            ('?', 0.04767759516835213),\n                            (';', 0.016911406069993973),\n                            (',', 0.009228551760315895),\n                            ('as', 0.00815183762460947),\n                            ('...', 0.0050266338512301445),\n                            ('-', 0.0027662296779453754),\n                            ('!', 0.002120816381648183),\n                            ('were', 0.0019135017646476626)]},\n 'NP/Z': {'ambiguous': [('.', 0.7722893953323364),\n                        (';', 0.10023866593837738),\n                        (':', 0.039806801825761795),\n                        ('?', 0.036088746041059494),\n                        (',', 0.024260535836219788),\n                        ('!', 0.00868281815201044),\n                        ('...', 0.005542231258004904),\n                        ('and', 0.0007047722465358675),\n                        ('was', 0.0005894345813430846),\n                        ('said', 0.00047553572221659124),\n                        ('â€¦', 0.0004526667471509427),\n                        ('\"', 0.0004359260783530772)],\n          'un-ambiguated': [('was', 0.2865143120288849),\n                            ('.', 0.17061203718185425),\n                            ('passed', 0.07756759971380234),\n                            (':', 0.0730673149228096),\n                            ('said', 0.02699584886431694),\n                            ('would', 0.01955598033964634),\n                            (';', 0.0192155372351408),\n                            ('is', 0.015060055069625378),\n                            ('amended', 0.012215839698910713),\n                            (',', 0.011330142617225647),\n                            ('read', 0.010200352407991886),\n                            ('became', 0.010115385986864567)]},\n 'MVRR': {'ambiguous': [('.', 0.9299089312553406),\n                        (';', 0.05484796315431595),\n                        ('to', 0.0025956383906304836),\n                        ('?', 0.002324273344129324),\n                        (':', 0.002262246562168002),\n                        ('!', 0.002220958936959505),\n                        (',', 0.0009328243904747069),\n                        ('out', 0.0005185769405215979),\n                        ('and', 0.00047631331835873425),\n                        ('over', 0.00027681421488523483),\n                        ('off', 0.0002687510568648577),\n                        ('...', 0.00023725115170236677)],\n          'un-ambiguated': [('.', 0.7884126901626587),\n                            ('?', 0.07973551750183105),\n                            (';', 0.056291092187166214),\n                            ('!', 0.015433131717145443),\n                            (':', 0.01527879387140274),\n                            ('was', 0.01258365623652935),\n                            ('...', 0.007526502013206482),\n                            (',', 0.007291005924344063),\n                            ('is', 0.0029977497179061174),\n                            ('for', 0.0012697605416178703),\n                            ('of', 0.0008813065360300243),\n                            ('-', 0.0008406759006902575)]}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ratio(a, b):\n",
    "    return a/b\n",
    "\n",
    "def get_ratios(filler_results, fillers):\n",
    "    ratios = {}\n",
    "    for sentence_type in filler_results:\n",
    "        for clarity in ['ambiguous', 'un-ambiguated']:\n",
    "            correct_mean = 0\n",
    "            incorrect_mean = 0\n",
    "            for accuracy in fillers:\n",
    "                for filler in fillers[accuracy]:\n",
    "                    filler_results[sentence_type][clarity][accuracy][filler]\n",
    "    return ratios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratios = get_ratios(filler_results)\n",
    "pprint(ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Behavior Exploration -- GPT2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np \n",
    "\n",
    "def score(model, tokens_tensor):\n",
    "    loss=model(tokens_tensor, labels=tokens_tensor)[0]\n",
    "    return np.exp(loss.cpu().detach().numpy())\n",
    "\n",
    "def runGPT(sentence_dict):\n",
    "    model = GPT2LMHeadModel.from_pretrained(models['GPT'])\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(models['GPT'])\n",
    "    filler_results = {}\n",
    "    top_preds = {}\n",
    "    model.eval()\n",
    "    for sentence_type in sentence_dict:\n",
    "        for sentence in sentence_type['sentences']:\n",
    "            # text = sentence.replace('[MASK]', '')\n",
    "            # with torch.no_grad():\n",
    "                # outputs = model(tokenizer.encode(text, add_special_tokens=False, return_tensors=\"pt\"))\n",
    "                # predictions = outputs[0][0, -1, :]\n",
    "                # print(len(predictions))\n",
    "                # print([tokenizer.decode([pred.item()]) for pred in predictions])\n",
    "\n",
    "            # next_token_logits = outputs[0]\n",
    "            # print(next_token_logits)\n",
    "            \n",
    "            filler_results[sentence] = {}\n",
    "            for filler in sentence_type['fillers']:\n",
    "                tokens_tensor = tokenizer.encode(sentence.replace('[MASK]', filler), add_special_tokens=False, return_tensors=\"pt\")\n",
    "                filler_results[sentence][filler] = score(model, tokens_tensor)\n",
    "    return filler_results, top_preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "filler_results, top_preds = runGPT(sentence_dict)\n",
    "print_header(\"Filler Results\")\n",
    "pprint(filler_results, sort_dicts=False)\n",
    "print_header(\"Model Predictions\")\n",
    "pprint(top_preds, sort_dicts=False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----------------------------------------------------------\nFiller Results\n----------------------------------------------------------\n{'The corrupt politician mentioned the bill [MASK]': {'is': 723.888,\n                                                      'was': 617.42737,\n                                                      'and': 574.95264,\n                                                      '.': 1431.8153,\n                                                      'to': 592.2486,\n                                                      'received': 1414.098},\n 'The corrupt politician that mentioned the bill [MASK]': {'is': 538.2617,\n                                                           'was': 508.98694,\n                                                           'and': 585.8596,\n                                                           '.': 1188.5912,\n                                                           'to': 596.9901,\n                                                           'received': 1133.8853},\n 'After the corrupt politician signed the bill [MASK]': {'is': 222.96799,\n                                                         'was': 190.7782,\n                                                         'and': 125.204796,\n                                                         '.': 323.2613,\n                                                         'to': 125.716484,\n                                                         'received': 361.25812},\n 'After the corrupt politician signed, the bill [MASK]': {'is': 168.80563,\n                                                          'was': 123.31906,\n                                                          'and': 226.08734,\n                                                          '.': 397.7348,\n                                                          'to': 217.28883,\n                                                          'received': 214.79462},\n 'The corrupt politician handed the bill [MASK]': {'is': 1001.32806,\n                                                   'was': 1096.115,\n                                                   'and': 556.5241,\n                                                   '.': 1347.8455,\n                                                   'to': 232.68866,\n                                                   'received': 1855.3574},\n 'The corrupt politician handed the bill later [MASK]': {'is': 1501.72,\n                                                         'was': 1109.7532,\n                                                         'and': 719.0449,\n                                                         '.': 1623.7888,\n                                                         'to': 606.14703,\n                                                         'received': 1570.9684}}\n\n\n----------------------------------------------------------\nModel Predictions\n----------------------------------------------------------\n{}\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}